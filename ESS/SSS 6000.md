# SSS 6000
![Image](https://github.com/user-attachments/assets/3460225d-a948-4d40-9c40-09391e1e6100)

➡️ 위: A (1번)
아래: B (2번)
* ESS3000 같은 경우엔 왼쪽이 A, 오른쪽이 B // 잊지 말 것

- 보통 EMS 서버와 함께 납품
  - EMS 서버의 용도
    - Quorum node, GUI Server 역할 (이는 ESS 6000에 직접 설치하면 안 됨!)
    - Call home server (보통 사용하지 않음)
    - 두 개 중 한 쪽이 떨어져 나가면 클러스터가 죽기 때문에 EMS가 존재
    - 관리, GUI 모두 EMS로 올라가며, 서비스들은 모두 EMS에 설치해야 함
    - SSS 6000은 스토리지 I/O를 위한 장비로 아무것도 설치하면 안 됨
- IBM Switch와 함께 사용 (vlan, shared vlan)
- port는 무조건 정해진 가이드대로 꽂아야 함

### SSR
- 깡통 리눅스가 깔려오기 때문에 SSR을 이용해 ip를 설정
- `ip a`를 통해 ssr 확인 가능
- 두 노드의 ssr ip가 같기 때문에 접속 방법은 구글에서 ibm sss 6000 ssr port 검색 또는 guide 참고

### ESS 3500
- ems의 campus는 고객 내부망
- management가 위, shared bmc가 아래에 꽂힘
- ssr은 선 꽂으면 포트가 살아남 (양쪽으로 ip가 똑같이 생성됨)

### 💡IBM Knowledge Center
- Documentation > Storage Scale System > 버전별로 가이드 나옴
- Hardware guide, Software guide 확인 가능

#### <mark style='background-color: #FFC0CB'> 참고 </mark>
- 초반에 설치하고 나서 hostname 잡아줘야 함 (기본: localhost)
- ib를 4개씩 양쪽 총 8개를 꽂고 나서 ip를 모두 잡아주진 않아도 됨
  - infini band는 active-active 개념이 없기 때문에 bonding으로 묶어서 ip가 하나로 보이게 만들어주면 됨
  - gpfs는 포트로 통신하기 때문에 포트 설정해주면 됨

---

## 모르는 용어
- BAY : 디스크 장착하는 슬롯 (2U 서버가 24BAY일 경우, 최대 24개 드라이브 꽂을 수 있음)
- GDS : GPU와 스토리지 간 데이터 전송 최적화하는 기술
    - CPU를 거의 거치지 않고 스토리지에서 GPU 메모리로 바로 전송 가능
    - 속도 향상, CPU 부담 감소, 대규모 데이터 처리 최적


## ESS?
- RAID + erasure coding 적용
    - 데이터 손상이나 장애 발생 시 투명하게 복구
    - 성능 저하 최소화
    - 페타바이트 규모 파일시스템에서도 HA 유지

---

# ESS 3500 
<img width="730" height="576" alt="image" src="https://github.com/user-attachments/assets/700efd30-0e67-4fad-86b8-daa62453c991" />

- **최대 초당 91기가바이트 성능**을 낸다. (NVMe, 4세대 PCIe, HPR 어댑터 조합 결과)
- 인클로저를 NVMe 기반 SSD와 하드디스크를 하이브리드 방식으로 구성할 수 있다.
- 하이브리드 구성 시 1식 기준, 최대 17 페타바이트 용량 확보 가능
- 2U 크기의 NVMe 기반 SSD가 장착되는 인클로저는 최대 737 테라바이트의 용량 수용 가능!!
- ESS3500은 2세대, 3세대 ESS(3000, 3200, 5000)과 호환 가능
- GDS (GPU Direct Storage) 지원 (성능 굿)
- 최대 4개의 HDD Shelves를 지원. but, 모든 HDD는 동일한 용량이어야 함

<BR>

<img width="544" height="528" alt="image" src="https://github.com/user-attachments/assets/fac43d5c-28c7-4f85-8d29-983f8edc70b8" />

- 스펙트럼 스케일은 AI, 빅데이터, 하이브리드 클라우드를 수용하는 쪽으로 진화 중.

- ESS는 GPU Direct Storage 지원
    - 고성능  GPU와 네트워킹 기술을 적용한 클러스터에서 스토리지가 성능 병목 구간이 되는 것을 방지하는 기술

- 빅데이터의 경우 대용량 로컬 캐시를 통해 처리량을 높였음
- 클라이언트 노드의 SSD나 플래시 드라이브를 로컬 캐시로 사용 가능하다.
- 이에 따라 CPU가 데이터를 기다리는데 소비하는 시간과 네트워크 및 스토리지 자원의 부하를 줄여 데이터 입출력 성능을 가속화한다.


<BR>

<img width="1185" height="500" alt="image" src="https://github.com/user-attachments/assets/ffeea808-ea9a-45ff-94ee-1f834f54c61e" />

- 하이브리드 클라우드의 경우 HCI를 위한 이레이저 코드 에디션이 추가되었다.
- ESS는 오픈스택 Swift 및 아마존 S3와 연계를 위한 프로토콜을 지원하여 퍼블릭 클라우드를 대상으로 수평적인 확장도 가능하다. 

✅ 정리
> ESS는 하이브리드 클라우드 환경에서 **데이터 보호와 클라우드 연계**를 지원하며, 스토리지를 확장하기도 쉬움

<BR>

### 🌟 도입 사례
1. 기초과학연구원
- 단일 네임스페이스로 여러 파일시스템을 연계하고 관리하기 위해 ESS 5000 스토리지와 최신 LTO 기술이 적용된 TS4500을 도입하였다. 

<BR>

### 사양
<img width="714" height="451" alt="image" src="https://github.com/user-attachments/assets/a87d2ef0-56a6-4e97-a44f-0126d15e1b03" />

<img width="705" height="579" alt="image" src="https://github.com/user-attachments/assets/12a51a0f-a3b1-4124-8225-5bedaddfd723" />

---

# IBM LTO-9 테이프 기술
- 18TB
- 최대 400MBpS
- 12Gb SAS 호스트 인터페이스
- 8Gb FC 호스트 인터페이스


# IBM Diamondback
<img width="121" height="400" alt="image" src="https://github.com/user-attachments/assets/b5f4f9af-cfe0-44a8-a3a0-a23a58aa5fa8" />

- IBM LTO-9 테이프 기술 : 2021년 기술 출시
- cold data 보관
  
| 사양 | 내용 |
| ------------ | ---------------- |
| 네이티브 데이터 전송률  | 400 MB/s |
| 네이티브 용량 | 18TB |
| 드라이브 수량 | 최대 14개  |
| 압축률 | 2.5:1  |
| 테이프 카트리지 개수  | 최대 1,584개 (1548개 권장) |
| 최대 용량(압축률 적용) | 69.6PB (카트리지 1,548개)   |
| 서비스 매거진 | 매거진 1개 및 카트리지 슬롯 10개   |
| 관리 | GUI, SCSI 및 REST API  |
| EIA 공간 | 42U |
| 높이 | 약 2M |
| 너비 | 커버 포함 약 60cm |
| 최대 무게 | 785kg |





