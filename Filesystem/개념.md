## AI 기반 병렬파일시스템

- 클러스터 환경에서 단일 스토리지의 한정된 I/O 대역폭을 확장하는 방법
  - `NAS`: Network Attached Storage
  - `SAN`: Storage Area Network
- 위의 환경에서 여러 대의 스토리지에 파일을 분산하여 저장하고 각 파일에 위치, 상태정보를 저장하는 메타데이터를 배치한다.

- Lustre와 GPFS는 대규모 클러스터 환경에서 사용되는 대표적인 병렬파일시스템으로 계산 시스템에 마운트되어 스토리지와 연결된 여러 개의 인터커넥트 대역폭을 통해 분산되어 I/O를 처리한다.

<br>

---

### 1. `Lustre` (Linux + Cluster)
![Image](https://github.com/user-attachments/assets/b884d63f-3532-446f-aadd-f8c2ea9e5a29)
- 대규모 클러스터 환경을 지원할 수 있는 객체 기반의 파일시스템
- 메타데이터 서버를 통해 데이터의 인덱스 정보를 얻은 후 실제 I/O 처리는 데이터 서버와 직접 통신하게 됨
- 기존의 하드웨어 위에서 동일하게 설치하여 구동할 수 있으며, `OSD(Object-based Storage Disk)` 기술을 사용하여 파일 I/O 메타데이터 연산을 따로 분리하였음

<br>

#### 1) Object-based Storage Disk
![Image](https://github.com/user-attachments/assets/806ed77b-8098-4567-80d0-81f847342efb)
- Lustre 파일시스템은 기존의 파일과 블록기반의 인터페이스를 사용하는 공유 파일시스템이 갖는 단점을 극복하기 위해 객체 기반의 인터페이스를 사용하여 확장성, I/O 성능, 보안성을 동시에 만족시킴
- NFS(파일인터페이스 기반 공유파일 시스템)
  - 보안성⬆️, 파일서버는 단 한대이기 때문에, 동시에 다수의 클라이언트에서 I/O를 요청할 경우 성능의 저하 피할 수 없음
- SAN(블록인터페이스 기반 공유파일 시스템)
  - 성능, 확장성⬆️, I/O를 관리하고 메타데이터를 유지하는 파일서버 없이는 데이터 공유와 보안 측면에 한계 존재
- OSD에서는 파일-블록 매칭 연산을 OS의 역할에서 디스크의 역할로 분리시키고, 파일 I/O를 위한 공통적인 인터페이스를 디스크에 내장시킴
- 파일을 쓰거나 읽기를 원하는 클라이언트는 원하는 데이터 액세스를 위해 자신이 원하는 데이터가 있는 OSD들에게 공통적인 인터페이스를 사용하여 접근
- OSD가 공통적인 인터페이스를 제공함에 따라 클라이언트들은 다수의 OSD에 동시 접근이 가능하며, 이에 따라 I/O 성능과 확장성이 개선됨

<br>

#### 2) Lustre 클러스터 구성
- Lustre 파일시스템은 `MDS(Meta Data Server)`, `OSS(Object Storage Server)`, `클라이언트` 세 부분으로 구성
  ![Image](https://github.com/user-attachments/assets/f00def6f-c454-4168-89bf-c71a4fd317ca)
- Lustre 파일시스템에는 OSD를 Lustre에 적용하기 위해 일반적인 블록기반 디스크(e.g., HDD)가 컴퓨터의 도움을 받아 OSD 역할을 수행
  - 파일매니저의 역할을 수행하도록 도움을 주는 컴퓨터를 `MDS`, OSD의 역할을 할 수 있도록 도움을 주는 컴퓨터를 `OSS`라고 지칭
- 하나의 OSS에는 여러 개의 블록기반 디스크가 물리적으로 연결되어 각각의 디스크마다 OSD로서의 역할 수행 가능
  - 이 각각의 디스크를 OST(Object Storage Target), MDS에 연결된 각각의 디스크를 MDT(MetaData Target)라고 부름
- 구동 중에 새로운 OST를 동적으로 추가가 가능하며, 새로 추가된 OST는 자동적으로 MDS에 등록되어 사용가능
- `MDS`
  - 메타데이터를 클라이언트에게 제공
  - 파일 생성과 검색, 파일 속성 변경, OST로의 파일 I/O 리디렉션 등의 메타데이터 연산 수행
  - 클라리언트가 I/O 연산을 수행하기 위해서는 먼저 MDS에서 메타데이터 연산을 수행하여 액세스하기 원하는 파일이 저장된 OST에 대한 정보를 얻은 후, OST에 I/O를 요청할 수 있음
  - 파일시스템 메타데이터 변화와 클러스터 상태에 대한 기록을 저장해서 하드웨어나 네트워크의 이상으로 인한 복구를 할 수 있도록 해줌

<br>

 #### 3) LOV(Logical Object Volume)
- Lustre 클라이언트들에게 하나의 마운트 가능한 장치로 보이는 단위
- Lustre는 하나의 MDT와 다수의 OST를 하나로 묶어서 하나의 파일시스템을 제공하며, 클라이언트들은 모두 LOV의 이름을 사용하여 접근가능
- MDT에 대한 장애복구 기능을 사용할 경우 MDT를 2개로 구성하여, MDT 서버에 장애가 일어나 전체 파일시스템에 액세스가 불가능하게 되는 상황 방지할 수 있음

<br>

#### 4) Lustre 파일 전송
![Image](https://github.com/user-attachments/assets/8844b553-7912-4d54-a16f-6966d3a506aa)
<br>
1️⃣ 자신이 원하는 파일의 부분을 저장하고 있는 OST의 주소를 MDS에 요청
<br>
2️⃣ MDS로부터 자신이 원하는 파일이 있는 OST의 주소를 받게 되면, 이후에는 MDS와의 통신 없이 OST와만 직접적으로 I/O를 계속해서 수행
<br>
3️⃣ MDS로부터 받은 메타데이터를 바탕으로 OST에 I/O요청을 하면
<br>
4️⃣ OST와의 데이터 전송이 시작

<br>

---

<br>

### 2. GPFS
# gpfs 구조 사진 추가하기!!
- 다수의 노드로부터 기존의 POSIX API를 통해 동시적인 파일액세스를 가능하게 하는 블록 I/O 기반의 클러스터 파일시스템
- 기존 분산 파일시스템의 I/O 대역폭 이상의 성능을 내도록 설계되어, 큰 데이터 대역폭이 요구되는 환경에 적합
- 리눅스와 AIX의 OS를 지원함. 호환성과 확장성 굿

<br>

- 메타데이터 서버를 별도 구성할 수 있게 허용하여 파일시스템에 대한 메타데이터 액세스가 증가할 때 메타데이터 처리에 대한 병목현상을 해소
- 클라이언트 노드마다 파일 I/O를 위한 GPFS 전용 버퍼인 `Pagepool`을 할당하여 시스템의 상황에 영향 받지 않고, 일관성 있는 I/O 성능 유지 가능
- 안정성 위해 데이터와 메타데이터의 복사본을 만드는 것이 가능하며, 서버에 대한 장애복구 구성 가능 




 
